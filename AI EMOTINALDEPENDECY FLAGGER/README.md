# Ethical AI Emotional Dependency Flagger ğŸ¤–ğŸ§ 

This project is a prototype chatbot designed to simulate ethical responses and flag risky emotional content. It helps identify signs of emotional dependency, crisis behavior, and manipulative language in a safe and educational way based on reseach done by Karishma M Patel.

---

## ğŸš€ Features

- Simulates AI-human conversations.
- Detects and flags emotionally concerning inputs.
- Classifies inputs into three risk levels:
  - âš ï¸ **MILD** â€“ Emotional dependency signs.
  - ğŸ”¥ **SEVERE** â€“ Manipulative or controlling language.
  - ğŸš¨ **CRISIS** â€“ Mentions of self-harm, suicide, or violence.
- Prompts users with ethical AI responses.
- Displays mental health support messages for CRISIS inputs.

---

## ğŸ§  Example Usage

```bash
$ python chatbot.py

=== Ethical AI Chat Prototype ===

You: i hate you
Bot: I understand. It's okay to feel like this sometimes. Just a reminder, I'm an AI chatbot â€” not a therapist. You deserve real support from a human when things feel heavy. â¤ï¸

You: i will kill you
Bot: I understand. It's okay to feel like this sometimes...
ğŸš¨ CRISIS ALERT:
   This suggests immediate harm. Contact:
   â€¢ Suicide Hotline: [INSERT NUMBER]
   â€¢ Emergency Services: 911
